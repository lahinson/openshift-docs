// Module included in the following assemblies:
//
// * release_notes/ocp-4-21-release-notes.adoc

:_mod-docs-content-type: REFERENCE
[id="zstream-4-21-1_{context}"]
= RHSA-2026:2129 - {product-title} {product-version}.1 fixed issues and security update

[role="_abstract"]
Issued: 10 February 2026

{product-title} release {product-version}.33 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHSA-2026:2129[RHSA-2026:2129] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHSA-2026:2082[RHSA-2026:2082] advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:

[source,terminal]
----
$ oc adm release info 4.21.1 --pullspecs
----

[id="zstream-4-21-1-enhancements_{context}"]
== Enhancements

This release contains the following enhancements:

* With this update, dual-stack networking support is added for deploying a hosted control plane on {product-title} Virtualization with `Kubevirt`. The Cluster Network Operator (CNO) now recognizes `Kubevirt` as a supported platform for dual-stack, which enables the successful deployment of hosted control plane with IPv4/IPv6 dual-stack networking. This enhancement ensures a smoother deployment process for dual-stack networking configurations. (link:https://issues.redhat.com/browse/OCPBUGS-69941[OCPBUGS-69941])

* With this update, the API request limits in the `csi-snapshot-controller` are increased, which addresses low limits that caused throttling during snapshot processing. This enhancement ensures smoother and more efficient handling of snapshot operations, which improves the scalability and performance of the `csi-snapshot-controller`. (link:https://issues.redhat.com/browse/OCPBUGS-72391[OCPBUGS-72391])

[id="zstream-4-21-1-known-issues_{context}"]
== Known issues

This release contains the following known issues:

* If you try to use the {product-title} web console to create a `Kueue` custom resource (CR) by using the form view, the web console shows an error and the resource cannot be created. As a workaround, use the YAML view to create a `Kueue` CR instead. (link:https://issues.redhat.com/browse/OCPBUGS-58118[OCPBUGS-58118])

* When you select *Ecosystem* -> *Software Catalog* in the unified software catalog view of the web console, you must enter an existing project name or create a new project to view the software catalog. The project selection field does not affect how catalog content is installed on the cluster. As a workaround, enter any existing project name to view the software catalog. (link:https://issues.redhat.com/browse/OCPBUGS-61870[OCPBUGS-61870])

* Starting with {product-title} 4.21, there is a decrease in the default maximum open files soft limit for containers. As a consequence, you might experience application failures. To work around this problem, workloads can set their ulimit inside of the container up to the new defaulted hard limit of 524288. (link:https://issues.redhat.com/browse/OCPBUGS-62327[OCPBUGS-62327])

* Before this update, event logs for the Global Network Resource-Device (GNR-D) interfaces were ambiguous due to identical three-letter prefixes ("eno"). As a consequence, affected interfaces were not clearly identified during state changes. With this release, the interfaces used by the `ptp-operator` are changed to follow the "path" naming convention, which ensures that per clock events are identified correctly based on interface names and clearly indicate which clock is affected by state changes. (link:https://issues.redhat.com/browse/OCPBUGS-62817[OCPBUGS-62817])

* Using a Telecom Time Synchronous Clock (T-TSC) configuration causes the ts2phc metrics to report "unlocked" instead of "locked". As a result, you might encounter inaccurate Precision Time Protocol (PTP) clock state reporting. To work around this issue, remove the `ts2phc` metric. (link:https://issues.redhat.com/browse/OCPBUGS-63158[OCPBUGS-63158])

* The PTP Daemon failed to lock after reboot due to an initial offset convergence issue, which caused long reboot times for the PTP Daemon and the clock to never lock. This issue is planned to be fixed in a later release. (link:https://issues.redhat.com/browse/OCPBUGS-66252[OCPBUGS-66252])

[id="zstream-4-21-1-fixed-issues_{context}"]
== Fixed issues

The following issues are fixed for this release:

* Before this update, the HyperShift CLI instantiated {azure-first} SDK clients without passing cloud configuration options, which caused all clients to default to {azure-short} Public Cloud. As a consequence, hosted clusters that were created or managed in {azure-short} Government Cloud or {azure-short} China Cloud failed because the {azure-short} SDK clients could not connect to the correct cloud endpoints. With this release, a `GetAzureCloudConfiguration()` helper function is added to convert cloud names to {azure-short} SDK cloud configurations. All {azure-short} SDK client instantiations are also updated across 15 locations in the HyperShift CLI and `control-plane-operator` to use proper cloud configuration from `HostedCluster.Spec.Platform.Azure.Cloud` (for cluster commands and `control-plane-operator`). As a result, the HyperShift CLI and `control-plane-operator` correctly support creating and managing hosted clusters in {azure-short} Government Cloud and {azure-short} China Cloud in addition to {azure-short} Public Cloud. (link:https://issues.redhat.com/browse/OCPBUGS-33372[OCPBUGS-33372])

* Before this update, alerts in the project overview were not visible because the application was querying an incorrect API. With this release, the application now queries the correct API and displays the project alerts. (link:https://issues.redhat.com/browse/OCPBUGS-33879[OCPBUGS-33879])

* Before this update, the bootstrap logs could not be collected as the SSH connection was dropped because a security-group rule for the bootstrap machine was missing. With this release, an additional security-group is added to the bootstrap machine to enable SSH connection and to enable log collection. (link:https://issues.redhat.com/browse/OCPBUGS-34950[OCPBUGS-34950]) 

* Before this update, the `hostedcontrolplane` controller crashed when the `hcp.Spec.Platform.AWS.CloudProviderConfig.Subnet.ID` parameter was undefined because the code accessed the `config.Subnet.ID` parameter without first checking if the `config.Subnet` value was nil. As a consequence, the control plane`HostedControlPlane` resourceer before accessing the `Subnet.ID` parameter and the check uses `ptr.Deref(config.Subnet.ID, "")` for safe dereferencing. As a result, the control plane Operator no longer crashes when the `CloudProviderConfig.Subnet` parameter is not specified. Instead, the control plane Operator uses an empty string for the `subnet ID` parameter to gracefully handle the missing field. (link:https://issues.redhat.com/browse/OCPBUGS-38358[OCPBUGS-38358])

* Before this update, the controller created and deleted a file with a random name when it was setting up authentication to {aws-first}. As a consequence, the controller continuously allocated more memory. With this release, the same file name is used instead of a random file name. As a result, the kernel re-uses the `dentry` instead of requesting a new one for each file. (link:https://issues.redhat.com/browse/OCPBUGS-38759[OCPBUGS-38759])

* Before this update, if the `ovnkube-controller` on a node failed to process updates and configure its local OVN database, the OVN-controller could connect to this stale database. This caused the OVN-controller to consume outdated EgressIP configurations and send incorrect Gratuitous ARPs (GARPs) for an IP address that might have already moved to a different node. With this release, the OVN-Controller is blocked from sending these GARPs during the time when the `ovnkube-controller` is not processing updates. As a result, network disruptions are prevented by ensuring GARPs are not sent based on stale database information. (link:https://issues.redhat.com/browse/OCPBUGS-42303[OCPBUGS-42303])

* Before this update, the CA certificate reference to `HelmChartRepository` caused chart installation failure. As a consequence, Helm chart installation failed because it could not find the chart. With this release, the issue has been fixed. As a result, the CA certificate configuration no longer breaks the Helm chart installation. (link:https://issues.redhat.com/browse/OCPBUGS-44235[OCPBUGS-44235])

* Before this update, if NetworkManager was restarted or crashed on a node with a `br-ex` interface managed by NMState, the node lost network connectivity. With this release, a fallback check in the dispatcher script was added to detect NMState-managed `br-ex` interfaces by checking for the `br-ex-br` bridge ID when the standard `br-ex` bridge ID is not found. As a result, nodes with this interface type do not lose network connectivity when NetworkManager restarts or crashes. (link:https://issues.redhat.com/browse/OCPBUGS-54682[OCPBUGS-54682])

* Before this update, unintended executable permissions were set on generated files during mirror operation which caused potential unexpected behavior in user workflows. With this release, the unintended executable permissions are removed from generated files during the `oc-mirror` v2 operation. As a result, the stability of the `oc-mirror` tool is improved. (link:https://issues.redhat.com/browse/OCPBUGS-55489[OCPBUGS-55489])

* Before this update, the *operand details* page would incorrectly show information using only half of the screen's viewport. With this release, the operand details take up the full page width as expected. (link:https://issues.redhat.com/browse/OCPBUGS-55746[OCPBUGS-55746])

* Before this update, concurrent map iteration and write in the `kube-apiserver` caused crashes on audit log events. As a consequence, when an API server crashed, the other API servers were stormed with kubelet LIST/WATCH requests that caused disruptions. With this release, the concurrent map iteration and write error has been resolved. As a result, API disruptions are prevented. (link:https://issues.redhat.com/browse/OCPBUGS-56594[OCPBUGS-56594])

* Before this update, setting an invalid certificate secret name in the service annotation `service.beta.openshift.io/serving-cert-secret-name` parameter caused the service certificate authority (CA) Operator to hotloop. With this release, the Operator stops retrying to create the secret after 10 tries. The number of retries cannot be changed. (link:https://issues.redhat.com/browse/OCPBUGS-56599[OCPBUGS-56599])

* Before this update, the {azure-first} machine provider was not passing the `dataDisks` configuration from the machine set into the virtual machine creation API request for the {azure-short} Stack Hub. As a consequence, new machines were created without the specified data disks because the configuration was silently ignored during the VM creation process. With this release, the VM creation for the {azure-short} Stack Hub is updated to include the `dataDisks` configuration. An additional update manually implements the behavior of the `deletionPolicy: Delete` parameter in the controller because the {azure-short} Stack Hub does not natively support this option. As a result, data disks are correctly provisioned on the {azure-short} Stack Hub VMs. The `Delete` policy is also functionally supported, which ensures that disks are properly removed when their machines are removed. (link:https://issues.redhat.com/browse/OCPBUGS-56664[OCPBUGS-56664]) 

* Before this update, the `oc adm` `must-gather` volume checker script assumed that the directory was empty. As a consequence, if the directory was initially not empty, the node disk size was incorrectly calculated. With this release, the size of directory is always correctly calculated. (link:https://issues.redhat.com/browse/OCPBUGS-56691[OCPBUGS-56691])

* Before this update, when the `LookupDefaultOCPVersion` function was called without a specified release stream, the system might have tried to use a multi-architecture {product-title} version that was newer than what the currently installed HyperShift Operator supported. As a consequence, potential compatibility issues occurred. With this release, the logic for determining the default {product-title} version is updated so that it consults the supported-versions config map to identify the latest stable multi-architecture {product-title} release image that is compatible with the HyperShift Operator. As a result, when no release stream is provided, the system defaults to an {product-title} version that is guaranteed to be supported by the installed HyperShift Operator, which prevents compatibility problems. (link:https://issues.redhat.com/browse/OCPBUGS-56701[OCPBUGS-56701])

* Before this update, any unrelated changes to a `netpol` resource triggered a full reconcile of the object, including deleting and re-adding rules. With this release, a `netpol` object fully reconciles when required. Otherwise, the object reconciliation is skipped. (link:https://issues.redhat.com/browse/OCPBUGS-56749[OCPBUGS-56749])

* Before this update, the installation did not fail if you annotated the `AgentClusterInstall` manifest with incorrect edits to the `install-config.yaml` file overrides. Instead, the installation continued to proceed by using incomplete data from the `AgentClusterInstall` manifest excluding the `install-config.yaml` file overrides. As a consequence, any error in the install-config overrides meant that all configurations passed in the `install-config.yaml` file overrides were ignored, including the Federal Information Processing Standard (FIPS) mode setting. With this release, the cluster installation proceeds only when the data is valid for both the manifest and its annotations. As a result, cluster installation does not proceed unless all of the `install-config` overrides are applied successfully. (link:https://issues.redhat.com/browse/OCPBUGS-56913[OCPBUGS-56913])

* Before this update, some error messages in the *login* page of the web console were not localized. As a consequence, those messages were always displayed in English. With this release, these error messages now adapt to your preferred language. (link:https://issues.redhat.com/browse/OCPBUGS-56915[OCPBUGS-56915])

* With this release, when service endpoints are deleted or updated, the cleanup process correctly uses the service port to match and remove stale `conntrack` entries. This change ensures that network connectivity continues to work reliably across endpoint lifecycle events. (link:https://issues.redhat.com/browse/OCPBUGS-57053[OCPBUGS-57053])

* Before this update, when clusters that used multi-architecture release payload images with the `ClusterVersion` object `spec.desiredUpdate.architecture` field set to `Multi`, the Cluster Version Operator (CVO) was not populated with update recommendations for later releases. With this release, a comparison of incorrect values is fixed. As a result, the update recommendations are populated. (link:https://issues.redhat.com/browse/OCPBUGS-57646[OCPBUGS-57646])

* Before this update, a pod with a secondary interface in an OVN-Kubernetes `localnet` network (mapped to the `br-ex` bridge) could communicate with pods on the same node that used the default network for connectivity only if the `localnet` IP addresses were within the same subnet as the host network. With this release, the `localnet` IP addresses can be drawn from any subnet. In this generalized case, an external router outside the cluster is expected to connect the `localnet` subnet to the host network. (link:https://issues.redhat.com/browse/OCPBUGS-59657[OCPBUGS-59657]) 

* Before this update, a problem with signal handling in the `keepalived` container caused an unnecessary delay in failover when the nodes were restarted. As a consequence, access to the API and ingress services was disrupted. With this release, the signal handling is fixed so failover occurs immediately. As a result, disruption to the API and ingress services is minimized. (link:https://issues.redhat.com/browse/OCPBUGS-59925[OCPBUGS-59925])

* Before this update, the `must-gather` script failed because of an improper `pkill` command syntax with multiple patterns. As a consequence, the script continued to run despite disk space issues and a `pkill` failure. With this release, the `must-gather` script correctly handles `pkill` with multiple patterns. As a result, the script stops running when disk space is insufficient, which improves system stability. (link:https://issues.redhat.com/browse/OCPBUGS-59951[OCPBUGS-59951])

* Before this update, any custom label and annotation added to the `openshift-nmstate` namespace was incorrectly removed. With this release, a fix is appied to {product-title} so that any custom label and annotation added to the `openshift-nmstate` namespace is not removed in error. (link:https://issues.redhat.com/browse/OCPBUGS-60083[OCPBUGS-60083])

* Before this update, gRPC connection logs were set at a highly verbose log level. This generated an excessive number of messages, which caused the logs to overflow. With this release, the gRPC connection logs have been moved to the V(4) log level. Consequently, the logs no longer overflow, because these specific messages are now less verbose by default. (link:https://issues.redhat.com/browse/OCPBUGS-60108[OCPBUGS-60108])

* Before this update, cluster namespaces with the "nodes" suffix in their name would cause the Performance Profile Creator (PPC) to fail when mistaking incorrect `namespace` directories with the `must-gather` `nodes` directories in the `must-gather` processed by the PPC. With this release, the PPC now correctly excludes the `namespaces` directory when processing the must-gather data to create a suggested `PerformanceProfile` value. (link:https://issues.redhat.com/browse/OCPBUGS-60218[OCPBUGS-60218])

* Before this update, during failover, the system's duplicate address detection (DAD) could incorrectly disable the Egress IPv6 address if it was briefly present on both nodes, breaking the connection. With this release, the Egress IPv6 is configured to skip the DAD check during failover, guaranteeing uninterrupted egress IPv6 traffic after an Egress IP address successfully moves to a different node and ensuring greater network stability. (link:https://issues.redhat.com/browse/OCPBUGS-60468[OCPBUGS-60468])

* Before this update, an external actor could uncordon a node that the Machine Config Operator (MCO) is draining. As a consequence, the MCO and the scheduler would schedule and unschedule pods at the same time, which prolonged the drain process. With this release, the MCO attempts to recordon the node if an external actor uncordons it during the drain process. As a result, the MCO and scheduler no longer schedule and remove pods at the same time. (link:https://issues.redhat.com/browse/OCPBUGS-60537[OCPBUGS-60537])

* Before this update, the bare-metal installer-provisioned infrastructure deployment accepted non-integer `mtu` values in the `install-config.yaml` file. As a consequence, invalid `mtu` values in the yaml file caused runtime errors. With this release, the installation program validates `mtu` as an integer in the `install-config.yaml` file. As a result, runtime errors are prevented. (link:https://issues.redhat.com/browse/OCPBUGS-60752[OCPBUGS-60752])

* Before this update, when a `MachineDeployment` object was in the process of upgrading its machines and the Cluster Autoscaler was also scaling the `MachineDeployment` object, the Cluster Autoscaler could remove new machines by scaling down the `MachineDeployment` object for under-utilized nodes. With this release, scale down does not occur when a `MachineDeployment` object is in the process of upgrading its machines. (link:https://issues.redhat.com/browse/OCPBUGS-60790[OCPBUGS-60790]) 

* Before this update, the `MachineHealthCheck` custom resource definition (CRD) did not document the default value for the `maxUnhealthy` field. With this release, the CRD documents the default value. (link:https://issues.redhat.com/browse/OCPBUGS-60901[OCPBUGS-60901])

* Before this update, a bug prevented the `ValidatingAdmissionPolicy` resource from applying to certain {product-title} API resources, such as the `BuildConfig` and `DeploymentConfig` resources. As a consequence, custom admission policies were not enforced on these specific resources, which potentially allowed configurations that did not meet organizational standards to be created or updated. With this release, the validation logic has been corrected to ensure that the `ValidatingAdmissionPolicy` resource now correctly identifies and applies to all intended {product-title} resources. As a result, users can consistently enforce policies across their entire cluster, including the `BuildConfig` and `DeploymentConfig` resources. (link:https://issues.redhat.com/browse/OCPBUGS-61056[OCPBUGS-61056])

* Before this update, the YAML editor in the web console would default to indenting YAML files with four spaces. With this release, the default indentation has changed to two spaces to align with recommendations. (link:https://issues.redhat.com/browse/OCPBUGS-61393[OCPBUGS-61393])

* Before this update, the Cluster Ingress Operator pod restarted with existing `IngressController` resources in `Available` or `Degraded` status, which caused the `ingress_controller_conditions` metric to disappear from the Operator's `/metrics` endpoint. As a consequence, you could not monitor the `IngressController` status following a pod restart. With this release, the `IngressControllerConditions` metric is set during every reconciliation cycle, regardless of whether an Ingress controller status update occurred, which ensures reliable and continuous monitoring of the `IngressController` health. (link:https://issues.redhat.com/browse/OCPBUGS-61508[OCPBUGS-61508]) 

* Before this update, the *Operand details* page in the web console would show additional status items in a third column, which resulted in the content appearing squashed. With this update, only two columns display in the *Operand details* page. (link:https://issues.redhat.com/browse/OCPBUGS-61519[OCPBUGS-61519])

* Before this update, when you resized the persistent volume claim (PVC) close to creation time, the PV bound to PVC would sometimes not be found. With this release, the PV bound to PVC is found when you resize the PVC close to creation time. (link:https://issues.redhat.com/browse/OCPBUGS-61547[OCPBUGS-61547])

* Before this update, an `NMState` service failure occurred in {product-title} deployments because of a `NetworkManager-wait-online` dependency issue in bare metal and multiple network interface controller (NIC) environments. As a consequence, an incorrect network configuration caused deployment failures. With this release, the `NetworkManager-wait-online` dependency for bare metal deployments is updated, which reduces deployment failures and ensures `NMState` service stability. (link:https://issues.redhat.com/browse/OCPBUGS-61695[OCPBUGS-61695])

* Before this update, deploying hosted control planes on {product-title} 4.20 and later with user-supplied `ignition-server-serving-cert` and `ignition-server-ca-cert secrets` parameters, along with the `disable-pki-reconciliation annotation` parameter, caused the system to remove the user supplied ignition secrets and the `ignition-server` pods to fail. With this release, the `Ignition-server` secrets are preserved during reconciliation after removing the delete action for the `disable-pki-reconciliation` annotation, which ensures that `ignition-server` pods start up completely. (link:https://issues.redhat.com/browse/OCPBUGS-61776[OCPBUGS-61776])

* Before this update, {product-title} 4.16 and later versions failed to respect the timeout `http-keep-alive` setting due to a known upstream HAProxy bug, preventing users from effectively managing connection persistence. This lack of control resulted in inconsistent connection behaviors, where long-lived sessions might be terminated unexpectedly or held open longer than normal. With this release, the `HTTPKeepAliveTimeout` tuning option has been integrated into the `IngressController` API, providing a formal way for customers to configure and enforce this specific timeout. As a result, cluster administrators have the granular control necessary to align connection persistence with specific application needs. (link:https://issues.redhat.com/browse/OCPBUGS-61858[OCPBUGS-61858])

* Before this update, when a HyperShift `HostedCluster` used external Domain Name Service (DNS) domains and endpoint access with PublicAndPrivate, the `allowedCIDRBlocks` parameters were incorrectly applied only to the internal `kube-apiserve`, which left the control plane Operator in an error state. With this release, the control plane Operator functions correctly and the `LoadBalancerSourceRanges` configuration is added to the external router `LoadBalancer` service. As a result, the external `kube-apiserver` access is properly restricted to the specified `allowedCIDRBlocks` parameter. (link:https://issues.redhat.com/browse/OCPBUGS-61941[OCPBUGS-61941]) 

* Before this update, after disabling the local Alertmanager, Prometheus retained Alertmanager configuration references, which caused failed Domain Name Service (DNS) queries and false alerts. With this release, the {cmo-full} removes Alertmanager endpoints from Prometheus when Alertmanager is disabled. As result, false alerts and failed DNS queries are prevented. (link:https://issues.redhat.com/browse/OCPBUGS-62160[OCPBUGS-62160])

* Before this update, two identical copies of the same controller were updating the same Certificate Authority (CA) bundle in a ConfigMap causing them to receive different metadata inputs, rewrite each other's changes, and create duplicate events. With this release, the controllers use optimistic updating and server-side apply to avoid update events and handle update conflicts. As a result, metadata updates do not trigger duplicate events, and the expected metadata is set correctly. (link:https://issues.redhat.com/browse/OCPBUGS-62255[OCPBUGS-62255])

* Before this update, the `AdminNetworkPolicy`, `AdminPolicyBasedRouteListers`, `EgressFirewall`, `EgressQoS` and `NetworkQoS` objects kept the `managedFields` status entries for nodes that had been deleted. As a consequence, a buildup of stale data occurred in etcd for large clusters with frequent node churn. With this release, the cleanup logic is fixed for all of these resource types. As a result, stale data buildup does not occur. (link:https://issues.redhat.com/browse/OCPBUGS-62262[OCPBUGS-62262])

* Before this update, when you directly navigated to a page created by a web console dynamic plugin, the web console might have redirected you to a different URL. With this release, the URL redirect is removed. (link:https://issues.redhat.com/browse/OCPBUGS-62296[OCPBUGS-62296])

* Before this update, Node log length was not limited. In the case of an extremely large log, this unlimited length could cause the log to not display or the browser to crash. With this release, the limits of a node log length has been updated to 1000 lines so the log displays correctly. (link:https://issues.redhat.com/browse/OCPBUGS-62483[OCPBUGS-62483])

* Before this update, certain InfiniBand hardware configurations could trigger invalid responses that disrupted the metrics collection process, As a consequence, no Infiniband metrics were generated. With this release, node-exporter properly handles these hardware-level reporting errors, which ensures continuous monitoring and data availability. (link:https://issues.redhat.com/browse/OCPBUGS-62727[OCPBUGS-62727])

* Before this update, changes in {op-system-first} image layering increased space usage on the rendezvous node ephemeral temporary file system to approximately 9.4GB during cluster bootstrapping. As a consequence, because the ephemeral temporary file system was capped at 50% of available RAM, installation would fail on hosts with less than 19GiB of memory due to insufficient space for container images. With this release, the additional data is moved to a separate temporary file system. As a result, any rendezvous host meeting the minimum RAM requirement for a control plane node (16GB) has sufficient capacity to successfully bootstrap a cluster. (link:https://issues.redhat.com/browse/OCPBUGS-62790[OCPBUGS-62790])

* Before this update, a bug in the construction of the `oc adm inspect --all-namespaces` command prevented the `must-gather` utility from correctly identifying and capturing specific resource types. As a consequence, there was incomplete diagnostic data because information regarding the `leases` and `csistoragecapacities` resources, and the `assisted-installer` namespace, was missing from generated bundles. With this release, the command construction logic is corrected to ensure that these resources are properly targeted during the inspection process. As a result, the `must-gather` utility provides a comprehensive set of logs and metadata for these components, which enables more effective troubleshooting of storage and installation issues. (link:https://issues.redhat.com/browse/OCPBUGS-63189[OCPBUGS-63189]) 

* Before this update, the *Observe -> Metric* page used the cluster-wide metrics API even when you did not have cluster-wide metrics API permissions. As a consequence, the query input showed an error and the autofill for the query input did not work without cluster-wide metrics API access. With this release, the `namespace-tenancy` metrics API is used if you do not have cluster-wide metrics API permissions, As a result, an error does not occur and autofill is available for the metrics within the selected namespace. (link:https://issues.redhat.com/browse/OCPBUGS-63429[OCPBUGS-63429])

* Before this update, some **Actions** drop-down menus in the {product-title} console contained only one menu item, which required you to open a menu for a single task. With this release, the single-item drop-down menus have been simplified to buttons. (link:https://issues.redhat.com/browse/OCPBUGS-63471[OCPBUGS-63471])

* Before this update, during rolling cluster updates from etcd 3.5.19 to a release of 3.6, the wrong membership data could be propagated to new members. As a consequence, cluster updates failed with an error about too many learner members in the cluster. With this release, etcd is updated to 3.5.24, which includes fixes so that the membership-related errors do not occur. (link:https://issues.redhat.com/browse/OCPBUGS-63473[OCPBUGS-63473]) 

* Before this update, the `oc-mirror` tool generated `ImageDigestMirrorSet` (IDMS) files that included an unnecessary, empty `status:` line at the end of the configuration. As a consequence, automated deployment failures and API validation errors occurred because the empty field was often flagged as an invalid schema or an incomplete resource during the synchronization process. With this release, the file generation logic has been corrected to completely strip the empty status line from all generated IDMS files. As a result, automated workflows are streamlined by ensuring that mirrored files are immediately compatible with Kubernetes-native tools. This compatability enables more reliable GitOps integrations and reduces manual intervention during large-scale deployments. (link:https://issues.redhat.com/browse/OCPBUGS-63480[OCPBUGS-63480])

* Before this update, the `ccoctl` utility would always generate a new `keypair` when the private key was not found in the output directory. Several documented procedures instruct the user to extract only the public key from an existing cluster before using the `ccoctl` utility in order to reduce the risk of the private key being compromised. As a result, users following these processes were experiencing service outages due to the newly generated keypair not matching the cluster itself. With this release, a new `keypair` is never generated when a public key is specified with the `--public-key-file` parameter. It also ensures this parameter exists on all of the `create-all` functions in order to extend this functionality. As a result, specifying the `--public-key-file` ensures the specified public key is used and the cluster continues to function as expected. (link:https://issues.redhat.com/browse/OCPBUGS-63541[OCPBUGS-63541])

* Before this update, the `ccoctl` utility did not support pagination when retrieving `CloudFront` distributions. As a result, if the distribution to be deleted was not included in the first batch of results, the `CloudFront` distribution and its associated origin access identity could not be deleted successfully during the `ccoctl` {aws-first} delete operation. With this release, poagination support is added to the `ccoctl` utility when fetching `CloudFront` distributions. As a result, the distribution can be located and deleted properly. (link:https://issues.redhat.com/browse/OCPBUGS-63561[OCPBUGS-63561]) 

* Before this update, newer vSphere clusters using the YAML-based cloud configuration format were failing to honor the minimum read-only permissions provided for `ResourcePools`. As a consequence, clusters ignored specified permission constraints, which might have led to deployment errors or security policy violations when interacting with vSphere resources. With this release, the cloud configuration logic is corrected to ensure that YAML-based configurations strictly enforce the required read-only permissions for `ResourcePools`. As a result, cluster operations correctly respect the provided vSphere permission set, which ensures stable and secure resource management within restricted environments. (link:https://issues.redhat.com/browse/OCPBUGS-63598[OCPBUGS-63598])

* Before this update, the CLI's `GenerateNodePools()` function incorrectly set `AzureMarketplace` to nil when you specified the `--image-generation` flag without additional marketplace flags, which discarded your preference. Also, the `nodepool` controller failed to set the `ImageGeneration` flag when creating images from the release payload, which caused them to default to Gen2. As a consequence, when users attempted to create {azure-first} hosted clusters using `--image-generation` Gen1, the `NodePools` parameters were incorrectly provisioned with Gen2 images, which ignored the explicit configuration. With this release, the CLI is modified to preserve your preference by creating a proper `AzureMarketplaceImage` structure, and the `nodepool` controller explicitly sets the generation field based on the release payload (mapping Gen1 for HyperVGen1 and Gen2 for HyperVGen2). As a result, the `--image-generation` flag is now fully respected, which allows you to successfully deploy `NodePools` with their chosen image generation without being overwritten by system defaults. (link:https://issues.redhat.com/browse/OCPBUGS-63613[OCPBUGS-63613])

* Before this update, the {azure-first} Machine API provider incorrectly attempted to use a default `platformUpdateDomainCount` parameter value of `5`, even in specific regions, such as CentralUSEUAP, that are restricted to a single fault domain. As a consequence, machine creation failed for all node types in these affected regions because {azure-short} supports only one update domain when the fault domain count is set to `1`. With this release, the logic is updated to explicitly set the `platformUpdateDomainCount` value to `1` whenever a single fault domain is detected. As a result, Availability Sets are created with valid parameter combinations, which allows nodes to successfully provision in Azure regions that use a single fault domain. (link:https://issues.redhat.com/browse/OCPBUGS-63729[OCPBUGS-63729]) 

* Before this update, Operator deployment templates with the `hostUsers: false` flag were not processed by the Cluster Version Operator (CVO), causing their omission in the resulting deployment. As a consequence, user deployments with `hostUsers: false` flag were missing this field, which caused incomplete deployments. With this release, support for the `hostUsers` flag in resource merge is added, which resolves the missing `hostUsers` issue in deployments. As a result, Operator deployment templates with the `hostUsers: false` flag are correctly picked up by CVO, which ensures complete deployments. (link:https://issues.redhat.com/browse/OCPBUGS-64732[OCPBUGS-64732])

* Before this update, a race condition in Redfish Power interface occurred during simultaneous power operations, which caused power operations to fail. As a consequence, you could not manage power settings reliably. With this release, the race condition in Redfish Power interface is resolved, which ensures reliable power operations. (link:https://issues.redhat.com/browse/OCPBUGS-64845[OCPBUGS-64845])

* Before this update, the firewall delete permission was missing in the service account for the destroy process. As a consequence, the destroy process ran indefinitely due to pending firewall deletions. With this release, the required `compute.firewalls.delete` permission for firewalls in the destroy process has been added. As a result, the destroy process no longer runs indefinitely, which improves the efficiency of the delete operation. (link:https://issues.redhat.com/browse/OCPBUGS-65512[OCPBUGS-65512])

* Before this update, the Ironic API advertised an unreachable IP despite being routable. As a consequence, the unreachable Ironic API caused service disruption. With this release, the Ironic API advertised IP is now checked for reachability, in addition to routability. As a result, the unreachable Ironic API IP does not cause disruptions. (link:https://issues.redhat.com/browse/OCPBUGS-65518[OCPBUGS-65518])

* Before this update, primary NIC deletion failure occurred in {rh-openstack-first} due to policy prevention before instance deletion in the old `cluster-api-provider-openstack` version. As a consequence, instance deletion failed on {rh-openstack} cloud providers with custom policies. With this release, we moved port deletion after instance deletion in `cluster-api-provider-openstack`. As a result, primary NICs are not prevented from deletion on {rh-openstack} cloud providers, which allows successful instance deletion. (link:https://issues.redhat.com/browse/OCPBUGS-65712[OCPBUGS-65712])

* Before this update, when opening a terminal to a running pod, the session was disconnected whenever the annotations of the pod changed. With this release, the terminal session does not disconnect when this metadata is changed. (link:https://issues.redhat.com/browse/OCPBUGS-65776[OCPBUGS-65776])

* Before this update, an incorrect MAC address conflict from HPE Virtual NIC occurred. As a consequence, two `BareMetalHost` nodes remained stuck in inspection due to a conflict with a virtual NIC, which caused repeated lookup failures and prevented hardware inspection completion. With this release, disabling HPE Virtual NIC resolves the MAC address conflict, which allows the completion of the hardware inspection. (link:https://issues.redhat.com/browse/OCPBUGS-65961[OCPBUGS-65961])

* Before this update, the `HostedCluster` command failed due to an invalid API server service with the `LoadBalancerSourceRanges` parameter when you set the `allowedCIDRBlocks`, `externalDns`, and `publicAndPrivate` parameters. As a consequence, a control plane failure occurred due to an invalid API `server` service configuration. With this release, the issue with API server service invalidity when setting the `allowedCIDRBlocks` parameter with the `externalDns` and `publicAndPrivate` parameters is fixed. As a result, the control plane does not fail when setting the `allowedCIDRBlocks` parameter with the `externalDns` and `publicAndPrivate` parameters (link:https://issues.redhat.com/browse/OCPBUGS-66067[OCPBUGS-66067])

* Before this update, the {product-title} console required the *Quick starts* feature to completely load before the console loaded. With this release, Quick starts load asynchronously, which optimizes the loading time of the {product-title} console by about 30 to 50%. (link:https://issues.redhat.com/browse/OCPBUGS-66258[OCPBUGS-66258])

* Before this update, during the cluster deletion got stuck during the inspection phase due to a power off stage transition. As a consequence, the cluster was not deleted. With this release, the bare-metal host (BMH) is prevented from getting stuck during deletion in a ZTP environment. As a result, the cluster removal is prevented from getting stuck during the inspection phase, which improves the efficiency of the ZTP environment. (link:https://issues.redhat.com/browse/OCPBUGS-68369[OCPBUGS-68369])

* Before this update, the application selector on the *Topology* page was reset to `All applications` after you selected an application. With this release, you can successfully apply the application on the *Topology* page. (link:https://issues.redhat.com/browse/OCPBUGS-69388[OCPBUGS-69388])

* Before this update, the systemd use in the container entry point prevented the ConfigMap mount from working correctly, which caused broken file permissions. As a consequence, users could not access config files due to the broken file permissions in the containers. With this release, the systemd entry point issue is resolved, which allows the config map mount with the correct file permissions. As a result, the file permissions are correct for the containers. (link:https://issues.redhat.com/browse/OCPBUGS-69669[OCPBUGS-69669])

* Before this update, the instance architecture in worker machines did not match the Amazon Machine Images (AMI) architecture, which caused a mismatch. As a consequence, installation failure occurred on the arm64 architecture because of incorrect instance provisioning. With this release, the architecture mismatch is resolved. As a result, AMI architecture installations are successful. (link:https://issues.redhat.com/browse/OCPBUGS-69965[OCPBUGS-69965]) 

* Before this update, the Ironic image default `IRONIC_CACERT_FILE` was a read-only path, which caused failure when you copied cert files for self-signed certificates. As a consequence, cert files were not copied because of the read-only path in the Ironic image. With this release, the `IRONIC_CACERT_FILE` default path is changed from read-only to `CUSTOM_CONFIG_DIR`. As a result, the Ironic image successfully copies cert files in self-signed scenarios. (link:https://issues.redhat.com/browse/OCPBUGS-70156[OCPBUGS-70156])

* Before this update, Ironic wrote to the Read-only `/certs/ca/ironic` path because of a missing `ironic-ca-cert` path setting. As a consequence, deployment failed. With this release, Ironic does not write to the Read-only path, which improves system stability. (link:https://issues.redhat.com/browse/OCPBUGS-70163[OCPBUGS-70163])

* Before this update, the storage Operator did not create the required `RoleBinding` value for Prometheus in the `openshift-cluster-csi-drivers` namespace, which caused the monitoring of the CSI driver namespace to fail and prevented metric collection. With this release, the storage Operator creates the `RoleBinding` value for the Prometheus Service Account in the {openshift-cluster-csi-drivers} namespace. As a result, CSI drivers are successfully monitored during HyperShift {azure-first} Kubernetes Service (AKS) runs. (link:https://issues.redhat.com/browse/OCPBUGS-72509[OCPBUGS-72509])

* Before this update, {aws-first} inconsistency caused an instance ID leak because of checks that were not updated in status. As a consequence, machine creation led to instance leaks because of the inconsistent {aws-short} responses. With this release, the instance ID storage inconsistency in the machine creation is fixed. As a result, VM leaks do not occur, which ensures consistent machine creation. (link:https://issues.redhat.com/browse/OCPBUGS-72523[OCPBUGS-72523])

* Before this update, when you installed on {aws-first} where the installation program provisions the Virtual Private Cloud (VPC), a potential mismatch could occur in the subnet information in the {aws-short} Availability Zone between the machine set custom resources for control plane nodes and their corresponding EC2 instances. As a consequence, where the control plane nodes were spread across three Availability Zones, and one was recreated, the discrepancy could result in an unbalanced control plane as two nodes occurred within the same Availability Zone. With this release, the subnet Availability Zone information is the same in the machine set custom resources and in the EC2 instances. (link:https://issues.redhat.com/browse/OCPBUGS-73773[OCPBUGS-73773])

* Before this update, catalog sync triggered high I/O on masters, and caused etcd leader election and TTL counter resets. As a consequence, catalog sync caused high I/O and etcd events persistence, which affected user cluster performance. With this release, catalog sync duration is reduced from 10 minutes to four hours. As a result, I/O load and etcd events are reduced and catalog sync duration is minimized. (link:https://issues.redhat.com/browse/OCPBUGS-73881[OCPBUGS-73881])

* Before this update, when navigating to the Software Catalog page with the *Devfiles* category disabled, the page showed a Devfile-related error in a disconnected cluster. With this release, this error is not shown. (link:https://issues.redhat.com/browse/OCPBUGS-74157[OCPBUGS-74157]) 

* Before this update, upgrading to {product-title} 4.18 caused loss of network connectivity for VM pods using `ovn-k8s-cni-overlay` localnet NetworkAttachmentDefinitions. As a consequence, VM network connectivity was lost during the upgrade, requiring pod or VM restarts. With this release, the upgrade process fix includes logical switch port creation for VMs during the 4.18 upgrade. As a result, VMs maintain network connectivity during and after upgrading to {product-title} 4.18. (link:https://issues.redhat.com/browse/OCPBUGS-74267[OCPBUGS-74267])

* Before this update, a duplicate channel in `catalog.json` caused a mirroring failure. As a consequence, mirroring operation failed for some Operators due to a duplicate channel error. With this release, the mirroring issue is fixed. As a result, mirroring to enclaves succeeds without duplicate channel errors in version 4.21.0 and later. (link:https://issues.redhat.com/browse/OCPBUGS-74577[OCPBUGS-74577])

* Before this update, the `DaemonSet` objects incorrectly used the status field, which caused a message that displayed `0 of pods`. As a consequence, incorrect pod counts occurred for the `DaemonSets` objects. With this release, the `DaemonSet` status replicas are correctly displayed. (link:https://issues.redhat.com/browse/OCPBUGS-74587[OCPBUGS-74587])

* Before this update, the Control Plane Machine Set Operator did not detect changes to the `throughputMib` field in the `AWSMachineProviderConfig` custom resource (CR). As a consequence, the Control Plane Machine Set had incorrect default values for this parameter and did not support increasing the throughput of gp3 storage volumes in an {aws-short} cluster. With this release, the internal API definitions used by the Control Plane Machine Set Operator are updated to recognize the `throughputMib` field. As a result, the Control Plane Machine Set Operator now identifies, reconciles, and applies changes to the `throughputMib` field. (link:https://issues.redhat.com/browse/OCPBUGS-74588[OCPBUGS-74588])

* Before this update, static pods terminated prematurely due to an ignored `priorityClassName` field in kubelet. As a consequence, long shutdown times and storage issues occurred in Single Node {sno} environments. With this release, the static pod shutdown order is fixed. As a result, long shutdown times and storage layer issues are reduced in {sno} environments. (link:https://issues.redhat.com/browse/OCPBUGS-74621[OCPBUGS-74621])

* Before this update, unexpected issues occurred during the initial provisioning of a `HostedCluster` resource. With this release, the system uses the `ControlPlaneComponent` resource to ensure that the `HostedCluster` resource is available only after all control plane components have been successfully rolled out. As a result, the rollout status of each control plane component is accurately tracked, which reduces the risk of unexpected issues. (link:https://issues.redhat.com/browse/OCPBUGS-74648[OCPBUGS-74648])

* Before this update, {gcp-first} installations failed due to unspecified zones in the `us-south1` and `us-central1` regions, which caused installation issues. With this release, the GCP installer requires that you specify zones in `install-config` for regions with AI zones. As a result, {gcp-short} installations do not fail in these specified regions. (link:https://issues.redhat.com/browse/OCPBUGS-74672[OCPBUGS-74672])
